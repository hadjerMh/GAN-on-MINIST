# MNIST GAN (Generative Adversarial Network)

This code implements a Generative Adversarial Network (GAN) for generating handwritten digit images from the MNIST dataset. The GAN consists of two neural networks, a Generator and a Discriminator, competing against each other in a minimax game to generate realistic-looking images.

## Dependencies

The code requires the following dependencies:

- Python
- PyTorch
- NumPy
- Matplotlib
- torchvision

## Usage

1. **Data Loading**: The code loads the MNIST dataset from the specified `DATA_PATH` and applies necessary transformations (resizing, normalization) to the images.

2. **Network Architecture**:
   - **Generator**: The Generator is a convolutional neural network that takes a random noise vector as input and generates fake images. It consists of several convolutional transpose layers with batch normalization and ReLU activations.
   - **Discriminator**: The Discriminator is a convolutional neural network that takes an image as input and outputs a scalar value representing the probability that the image is real or fake. It consists of several convolutional layers with batch normalization, leaky ReLU activations, and a final sigmoid activation.

3. **Training Loop**:
   - The training loop iterates over the specified number of epochs (`EPOCH_NUM`).
   - In each iteration, the Discriminator is updated twice: once with real images, and once with fake images generated by the Generator.
   - The Generator is then updated to generate images that can fool the Discriminator into classifying them as real.
   - The Binary Cross-Entropy (BCE) loss is used to train both the Generator and the Discriminator.
   - Adam optimizer is used for updating the network parameters.
   - Training progress is printed every 50 iterations, showing the losses and the Discriminator's output on real and fake images.
   - The Generator's output on a fixed noise vector is saved periodically for visualization.

4. **Visualization**:
   - After each epoch, the code plots a grid of real images from the dataset and a grid of fake images generated by the Generator.
   - The Generator and Discriminator losses are also plotted over the epochs.

5. **Model Saving**:
   - After each epoch, the trained model (generator, discriminator, optimizers, and loss history) is saved to a specified path in your Google Drive.

## Configuration

You can modify the following hyperparameters in the code:

- `CUDA`: Set to `True` if you want to use a CUDA-enabled GPU for training, `False` for CPU.
- `DATA_PATH`: Path to the directory where the MNIST dataset will be downloaded.
- `BATCH_SIZE`: Batch size for training.
- `Z_DIM`: Dimension of the random noise vector input to the Generator.
- `G_HIDDEN`, `D_HIDDEN`: Number of hidden units in the Generator and Discriminator, respectively.
- `EPOCH_NUM`: Number of training epochs.
- `lr`: Learning rate for the optimizers.
- `seed`: Random seed for reproducibility.

## Usage

1. Mount your Google Drive by running the `drive.mount` command.
2. Specify the path in your Google Drive where you want to save the model using the `drive_save_path` variable.
3. Run the code, and the training loop will start, saving the model and plotting the images and losses after each epoch.

Note: The provided code assumes that you are running it in a Google Colab environment with access to a GPU. If you are running it locally or on a different environment, you may need to modify the code accordingly.
